{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Stream active learning cycle\r\n",
    "In this notebook, we will show how stream-based active learning strategies are used and compared them to one another. We showcase the methods available in the stream package. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import sys\r\n",
    "sys.path.append('../..')\r\n",
    "\r\n",
    "import numpy as np\r\n",
    "import sklearn\r\n",
    "import sklearn.datasets\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import copy\r\n",
    "from collections import deque\r\n",
    "from skactiveml.classifier import PWC\r\n",
    "from skactiveml.stream import RandomSampler, PeriodicSampler\r\n",
    "from skactiveml.stream import FixedUncertainty, VariableUncertainty, Split, PAL\r\n",
    "from skactiveml.stream.budget_manager import FixedBudget"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Initialize stream Parameters\r\n",
    "Before the experiments can start, we need to construct a random data set. For this, we specify the necessary parameters in the cell below. To make the experiment repeatable, we will use the random_state object to generate all other random seeds, such that we only need to explicitly specify a single random seed. Furthermore, we specify the length of the data stream (stream_length) and the size of the sliding window that defines the available training data (training_size)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# random state that is used to generate random seeds\r\n",
    "random_state = np.random.RandomState(0)\r\n",
    "# number of instances that are provided to the classifier\r\n",
    "init_train_length = 10\r\n",
    "# the length of the data stream\r\n",
    "stream_length = 10000\r\n",
    "# the size of the sliding window that limits the training data\r\n",
    "training_size = 1000"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Random seed generation\r\n",
    "The get_randomseed function simplifies the generation of a new random seed from a given random state object. For this notebook, this random state object will be always the random_state obejct defined above."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_randomseed(random_state):\r\n",
    "    return random_state.randint(2**31-1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Generate and initialize data set\r\n",
    "The next block initializes the tested data set. We use scikit-learn to generate a random dataset with our pre-defined stream length. The data set consists of multiple parts. X represents the location of the instance within the feature space. The class for each instance is denoted by y. \r\n",
    "For models that need at least some initial training data, we generate samples to train an initial model. These are denoted by the suffix \"_init\", while all data used within the active learning cycle are denoted by the suffix \"_stream\"."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X, y = sklearn.datasets.make_classification(n_samples=init_train_length + stream_length, random_state=get_randomseed(random_state), shuffle=True)\r\n",
    "X_init = X[:init_train_length, :]\r\n",
    "y_init = y[:init_train_length]\r\n",
    "X_stream = X[init_train_length:, :]\r\n",
    "y_stream = y[init_train_length:]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Initialize query strategies\r\n",
    "Next, we initialize the classifier and the base query strategies that we want to compare. To guarantee that the classifier is not affected by previous repetitions, we use factory functions to separate the classifier for each experiment run. Since all query strategies havea defaultr budget manage,r we use hat for the sake of simplicityy."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "clf_factory = lambda: PWC(classes=[0,1], random_state=get_randomseed(random_state))\r\n",
    "query_strategies = {\r\n",
    "    'RandomSampler': RandomSampler(random_state=get_randomseed(random_state)),\r\n",
    "    'PeriodicSampler': PeriodicSampler(random_state=get_randomseed(random_state)),\r\n",
    "    'FixedUncertainty': FixedUncertainty(clf=clf_factory(), random_state=get_randomseed(random_state)),\r\n",
    "    'VariableUncertainty': VariableUncertainty(clf=clf_factory(), random_state=get_randomseed(random_state)),\r\n",
    "    'Split': Split(clf=clf_factory(), random_state=get_randomseed(random_state)),\r\n",
    "    'PAL': PAL(clf=clf_factory(), random_state=get_randomseed(random_state))\r\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Start the active learning cycle\r\n",
    "After all, variables are initialized, we can start the experiment. The experiment loop below goes through all query strategies defined in query_strategies. For each experiment run, the average accuracy of the selected query strategies will be displayed. Lastly, the accuracy over time will be plotted."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for query_strategy_name, query_strategy in query_strategies.items():\r\n",
    "    clf = clf_factory()\r\n",
    "    # initializing the training data\r\n",
    "    X_train = deque(maxlen=training_size)\r\n",
    "    X_train.extend(X_init)\r\n",
    "    y_train = deque(maxlen=training_size)\r\n",
    "    y_train.extend(y_init)\r\n",
    "    # train the model with the initially available data\r\n",
    "    clf.fit(X_train, y_train)\r\n",
    "    # initialize the list that stores the result of the classifier's prediction\r\n",
    "    correct_classifications = []\r\n",
    "    for t, (x_t, y_t) in enumerate(zip(X_stream, y_stream)):\r\n",
    "        correct_classifications.append(clf.predict(x_t.reshape([1, -1]))[0] == y_t)\r\n",
    "        # check whether to sample the instance or not\r\n",
    "        sampled_indices = query_strategy.query(x_t.reshape([1, -1]), X=X_train, y=y_train)\r\n",
    "        if len(sampled_indices):\r\n",
    "            X_train.append(x_t)\r\n",
    "            y_train.append(y_t)\r\n",
    "            # train the classifier\r\n",
    "            clf.fit(X_train, y_train)\r\n",
    "    # calculate and show the average accuracy \r\n",
    "    print(\"Query Strategy: \", query_strategy_name, \", Avg Accuracy: \", np.sum(correct_classifications)/stream_length)\r\n",
    "    cumsum_correct_classifications = np.cumsum(correct_classifications)\r\n",
    "    # smoothing the accuracy for plotting\r\n",
    "    smoothing_window_length = 100\r\n",
    "    plt.plot((cumsum_correct_classifications[smoothing_window_length:]-cumsum_correct_classifications[:-smoothing_window_length])/smoothing_window_length, label=query_strategy_name)\r\n",
    "plt.legend()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4b214dd2c9a44f12a1833edb4921179114be61850b6456c57ef3370ef440c7a3"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('scikit-activeml': conda)",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}